{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c8fc68-93a9-43a8-868e-749a71ee601b",
   "metadata": {},
   "source": [
    "Our needs for our model include:\n",
    "- This needs to be a model able to process labels and predict prices based on different attributes.\n",
    "- Needs to predict fair market values of properties\n",
    "- Due to the dataset we plan to use for this project + the predictive nature of what I need, we will be using a supervised regressor model for the price baseline calculation.\n",
    "- This will allow us to create a general price baseline so that we can detect any major anomalies in the dataset for any anomalous pricing using predictive modeling\n",
    "- This model ideally should be able to catch nonlinear and linear relationships, as these would be the best \n",
    "\n",
    "Possible Models/Algorithms:\n",
    "- Ridge Regression: This is a linear regressor that penalizes large coefficients; this regressor focuses primarily on linear relationships and acts as a simple model for these relationships; however, it fails to capture nonlinear relationships well, and with the varying attributes of homes could be a major disadvantage\n",
    "- Random Forest Regressor: This is a regression model related to the popular Random Forest algorithm, using various trees to make classifications and regressions based on supervised learning concepts. This is more suited for nonlinear data models and are more suited for complex patterns. However, these are more susceptible to overfitting due to tree depth as well as slowing down the pipeline due to a more slow model to train.\n",
    "- LightGBM/XGBoost Regressor: This is a popular gradient boosting type regression model that focuses on capturing more subtle changes and more non-distributed variances. This generally has a higher accuracy and builds on top of each tree's predictions. This is a faster model then its counterpart, XGBoost, which does similar regressive analysis, but trades computative cost for accuracy.\n",
    "\n",
    "To decide between these various regressors, we have also created metric analysis code in order to do metric analysis and comparison to decide the best models to use for our dataset; Before this though, a few main patterns; due to data processing, these are generally non-linear, so therefore that means XGBoost/LightGBM (gradient regressors) or Random Forest Regressors have a much more likely chance of being useful due to their usage for nonlinear operations and predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb9efca1-9400-4693-9328-3e11a9bb4e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average property price: $456,236\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "df_filled = pd.read_csv(\"../exploratory_data_analysis/imputed.csv\")\n",
    "\n",
    "target = 'taxvaluedollarcnt'\n",
    "cols_to_exclude = [target, 'parcelid', 'logerror']\n",
    "features = [col for col in df_filled.select_dtypes(include=[np.number]).columns \n",
    "            if col not in cols_to_exclude]\n",
    "\n",
    "X = df_filled[features]\n",
    "y = df_filled[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Average property price: ${y_test.mean():,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9faa501a-5bdd-460e-8c3d-7b681aaa3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def evaluate_model(y_acc, y_pred, model):\n",
    "    mae = mean_absolute_error(y_acc, y_pred)\n",
    "    mse = mean_squared_error(y_acc, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_acc, y_pred)\n",
    "    mape = np.mean(np.abs((y_acc - y_pred) / y_acc)) * 100\n",
    "    \n",
    "    print(f\"{model} Performance:\")\n",
    "    print(f\"  R² Score: {r2:.4f}\")\n",
    "    print(f\"  RMSE: ${rmse:,.2f}\")\n",
    "    print(f\"  MAE: ${mae:,.2f}\")\n",
    "    print(f\"  MAPE: {mape:.2f}%\")\n",
    "    print(f\"  RMSE as % of avg price: {(rmse / y_acc.mean()) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b26521e-c491-4c73-a469-559e84177d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Performance:\n",
      "  R² Score: 0.9978\n",
      "  RMSE: $25,221.01\n",
      "  MAE: $2,710.91\n",
      "  MAPE: 0.92%\n",
      "  RMSE as % of avg price: 5.53%\n"
     ]
    }
   ],
   "source": [
    "# here, models for price prediction will be trained using their library counterparts, and from there, dependent on the errors \n",
    "# and r^2 score, we would decide which one is best through data analysis and the eval function\n",
    "\n",
    "# Linear Regressor\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_lr_pred = lr.predict(X_test_scaled)\n",
    "evaluate_model(y_test, y_lr_pred, \"Linear Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22faa8c7-2c55-4412-8709-18f86d9f560e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regressor Performance:\n",
      "  R² Score: 0.9978\n",
      "  RMSE: $25,222.55\n",
      "  MAE: $2,715.71\n",
      "  MAPE: 0.93%\n",
      "  RMSE as % of avg price: 5.53%\n"
     ]
    }
   ],
   "source": [
    "# Ridge Regressor\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "y_ridge_pred = ridge.predict(X_test_scaled)\n",
    "evaluate_model(y_test, y_ridge_pred, \"Ridge Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ace3b489-1106-462c-ac7f-34ed9a8c15d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Performance:\n",
      "  R² Score: 0.9942\n",
      "  RMSE: $41,082.64\n",
      "  MAE: $4,926.09\n",
      "  MAPE: 0.65%\n",
      "  RMSE as % of avg price: 9.00%\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "rand_for = RandomForestRegressor(n_estimators=100, max_depth=20, min_samples_split=5, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "rand_for.fit(X_train, y_train)\n",
    "y_rf_pred = rand_for.predict(X_test)\n",
    "evaluate_model(y_test, y_rf_pred, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2423750-126f-4230-8377-ef7b310fe1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Performance:\n",
      "  R² Score: 0.9962\n",
      "  RMSE: $33,123.84\n",
      "  MAE: $9,247.36\n",
      "  MAPE: 2.29%\n",
      "  RMSE as % of avg price: 7.26%\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "gbr.fit(X_train, y_train)\n",
    "y_gbr_pred = gbr.predict(X_test)\n",
    "evaluate_model(y_test, y_gbr_pred, \"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a7948a5-74ce-435c-9ac5-51478646ba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Performance:\n",
      "  R² Score: 0.9041\n",
      "  RMSE: $166,787.54\n",
      "  MAE: $17,743.59\n",
      "  MAPE: 2.95%\n",
      "  RMSE as % of avg price: 36.56%\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "lgbm = LGBMRegressor(n_estimators=100,learning_rate=0.1, random_state=42, verbose=-1)\n",
    "lgbm.fit(X_train, y_train)\n",
    "y_lgbm_pred = lgbm.predict(X_test)\n",
    "evaluate_model(y_test, y_lgbm_pred, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7147743-9a2b-48bb-975a-6a3aab8d4efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Regressor Performance:\n",
      "  R² Score: 0.8769\n",
      "  RMSE: $188,944.12\n",
      "  MAE: $17,437.44\n",
      "  MAPE: 3.01%\n",
      "  RMSE as % of avg price: 41.41%\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_xgb_pred = xgb.predict(X_test)\n",
    "evaluate_model(y_test, y_xgb_pred, \"XGB Regressor\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
